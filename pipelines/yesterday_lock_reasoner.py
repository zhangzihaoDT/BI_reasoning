"""
æ­¤è„šæœ¬æ˜¯ yesterday_lock.py çš„ Reasoner å¢å¼ºç‰ˆã€‚
å®ƒä¿ç•™äº†åŸæœ‰çš„ DSL æ‰§è¡Œå›¾é€»è¾‘ï¼Œä½†åœ¨æœ€åå¼•å…¥ DeepSeek Thinking Mode (deepseek-reasoner)
å¯¹åˆ†æç»“æœï¼ˆåŸºçº¿ã€è¶‹åŠ¿ã€ç»“æ„ã€å¼‚å¸¸ä¿¡å·ï¼‰è¿›è¡Œæ·±åº¦è§£è¯»ï¼Œç”Ÿæˆè‡ªç„¶çš„ä¸šåŠ¡æ—¥æŠ¥ã€‚
"""
import sys
import os
import argparse
import json
import requests
import pandas as pd
import time
from typing import List, Dict, Any, Tuple

# Add project root to sys.path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from agents.execution_graph import build_execution_graph

def _load_api_key():
    env_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), ".env")
    if os.path.exists(env_path):
        with open(env_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip() and not line.startswith('#'):
                    if 'deepseek=' in line:
                        return line.split('=', 1)[1].strip()
                    if 'deepseek =' in line:
                            return line.split('=', 1)[1].strip()
    return None

API_KEY = _load_api_key()

def _parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser()
    p.add_argument("--date", type=str, help="Single date to analyze (YYYY-MM-DD or 'yesterday')")
    p.add_argument("--start", type=str, help="Start date for range analysis (YYYY-MM-DD)")
    p.add_argument("--end", type=str, help="End date for range analysis (YYYY-MM-DD)")
    args = p.parse_args()
    
    if not args.date and not args.start:
        args.date = "yesterday"
        
    return args

def calculate_risk(signals: List[Dict]) -> Dict[str, Any]:
    """
    æ ¹æ®ä¿¡å·è®¡ç®—é£é™©ç­‰çº§ï¼ˆå¤ç”¨ yesterday_lock.py çš„é€»è¾‘ï¼‰
    """
    risk_score = 0
    reasons = []

    for signal in signals:
        if signal.get('type') == 'anomaly_decision':
            if signal.get('anomaly_detected'):
                risk_score += 2
                reasons.append(f"è¶‹åŠ¿å¼‚å¸¸: {signal.get('metric')} ({signal.get('flag')})")
        
        elif signal.get('type') == 'distribution_signal':
            if signal.get('status') == 'abnormal':
                risk_score += 2
                reasons.append(f"åˆ†å¸ƒåç§»: {signal.get('metric')} (å·®å¼‚è¯„åˆ†: {signal.get('score'):.2f})")
            elif signal.get('status') == 'warning':
                risk_score += 1
                reasons.append(f"åˆ†å¸ƒé¢„è­¦: {signal.get('metric')}")
        
        elif signal.get('type') == 'data_quality_signal':
             if signal.get('status') == 'warning':
                risk_score += 1
                reasons.append(f"æ•°æ®è´¨é‡: {signal.get('message')}")

    if risk_score == 0:
        level = "Low"
        icon = "ğŸŸ¢"
    elif risk_score <= 2:
        level = "Medium"
        icon = "ğŸŸ¡"
    else:
        level = "High"
        icon = "ğŸ”´"

    return {
        "level": level,
        "score": risk_score,
        "reasons": reasons,
        "icon": icon
    }

def call_deepseek_reasoner(context_data: Dict[str, Any], prompt_type: str = "daily") -> Tuple[str, Dict[str, Any]]:
    """
    è°ƒç”¨ DeepSeek Reasoner æ¨¡å‹ç”Ÿæˆåˆ†ææŠ¥å‘Š
    Returns: (content, metrics)
    """
    if not API_KEY:
        return "âš ï¸ Error: DeepSeek API Key not found in .env", {}

    base_url = "https://api.deepseek.com"
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }

    # å°†å¤æ‚å¯¹è±¡è½¬æ¢ä¸º JSON å­—ç¬¦ä¸²ä»¥ä¾¿ LLM ç†è§£
    data_str = json.dumps(context_data, ensure_ascii=False, indent=2, default=str)

    if prompt_type == "daily":
        system_prompt = """ä½ æ˜¯ä¸€ä½â€œæ•°æ®ä¾¦æ¢â€ã€‚è¯·æ ¹æ®æä¾›çš„ç»è¥æ•°æ®ï¼ˆé£é™©è¯„ä¼°ã€æ ¸å¿ƒæŒ‡æ ‡ã€åŒç¯æ¯”ã€å¼‚å¸¸ä¿¡å·ã€ç»“æ„æ‹†è§£ã€åˆ†å¸ƒç‰¹å¾ï¼‰ï¼Œç”Ÿæˆä¸€ä»½**æç®€**ä¸”**é«˜å¯†åº¦**çš„ã€æ¯æ—¥ç»è¥è¯Šæ–­ã€‘ã€‚

**åŸåˆ™ï¼š**
1. **ç»“è®ºå…ˆè¡Œ**ï¼šç›´æ¥å¼•ç”¨ `risk_assessment` ä¸­çš„é£é™©ç­‰çº§å’ŒåŸå› ã€‚
2. **æ‹’ç»åºŸè¯**ï¼šä¸è¦å†™â€œæ•°æ®è¡¨æ˜â€ã€â€œç»è¿‡åˆ†æâ€ç­‰å«è¯ã€‚
3. **å…³é”®ä¿¡æ¯**ï¼šå¿…é¡»åŒ…å«å…·ä½“çš„ `sales` æ•°å€¼ã€åŒç¯æ¯”å˜åŒ–ã€ä»¥åŠå…·ä½“çš„ `signals` è¯¦æƒ…ã€‚

**è¾“å‡ºæ¨¡æ¿ï¼š**

## {risk_icon} è¯Šæ–­ç»“è®ºï¼šé£é™© {risk_level}
**æ ¸å¿ƒæ•°æ®**ï¼šé”€é‡ {sales} ({mom_str}, {wow_str})ã€‚
**é£é™©åˆ¤å®š**ï¼š{risk_reasons_str}ï¼ˆè‹¥æ— é£é™©åˆ™å†™â€œå„é¡¹æŒ‡æ ‡è¿è¡Œå¹³ç¨³â€ï¼‰ã€‚

## ğŸ” å¼‚åŠ¨å½’å› 
**1. ç»“æ„æ‹†è§£**ï¼š{top_contributor} å æ¯” {top_share}ï¼Œ{change_desc}ã€‚
**2. åˆ†å¸ƒç‰¹å¾**ï¼š{distribution_desc}ã€‚
**3. å¼‚å¸¸ä¿¡å·**ï¼š
- {signal_1}
- {signal_2}
*(è‹¥æ— ä¿¡å·åˆ™ä¸æ˜¾ç¤ºæ­¤å°èŠ‚)*

**æ³¨æ„**ï¼š
- æ›¿æ¢æ¨¡æ¿ä¸­çš„ {...} ä¸ºå®é™…æ•°æ®ã€‚
- å¦‚æœ risk_level ä¸º Highï¼Œè¯·ä½¿ç”¨ä¸¥è‚ƒè­¦ç¤ºè¯­æ°”ã€‚
"""
    else: # range summary
        system_prompt = """ä½ æ˜¯ä¸€ä½â€œè¶‹åŠ¿æ•æ‰‹â€ã€‚è¯·æ ¹æ®åŒºé—´å†…çš„æ¯æ—¥æ ¸å¿ƒæŒ‡æ ‡ä¸å¼‚å¸¸ä¿¡å·ï¼Œç”Ÿæˆä¸€ä»½**é«˜å¯†åº¦**çš„ã€åŒºé—´ç»è¥è½¨è¿¹ç»¼è¿°ã€‘ã€‚

**åŸåˆ™ï¼š**
1. **å®è§‚è§†è§’**ï¼šå…³æ³¨æ•´ä½“è¶‹åŠ¿ï¼ˆä¸Šå‡/ä¸‹é™/éœ‡è¡ï¼‰ï¼Œè€Œéæ¯æ—¥æµæ°´è´¦ã€‚
2. **å¼‚å¸¸é©±åŠ¨**ï¼šé‡ç‚¹å¤ç›˜åŒºé—´å†…çš„â€œå¼‚å¸¸ç‚¹â€ï¼ˆé«˜é£é™©æ—¥æœŸã€çªå˜ç‚¹ï¼‰ã€‚
3. **æç®€è¾“å‡º**ï¼šæ‹’ç»åºŸè¯ã€‚

**è¾“å‡ºæ ¼å¼ï¼š**

**1. è½¨è¿¹æ¦‚è§ˆ**
[åŒºé—´æ€»é‡] [è¶‹åŠ¿å½¢æ€ï¼šå¦‚â€œå…ˆæŠ‘åæ‰¬â€] [å…³é”®æå€¼ï¼šæœ€é«˜/æœ€ä½æ—¥]ã€‚

**2. å¼‚å¸¸å¤ç›˜**
- [æ—¥æœŸ]: [å¼‚å¸¸æè¿°] (å¼•ç”¨ Z-Score æˆ– é£é™©ç­‰çº§)ã€‚
- [æ—¥æœŸ]: [å¼‚å¸¸æè¿°]ã€‚
*(è‹¥æ— å¼‚å¸¸ï¼Œå†™â€œåŒºé—´å†…è¿è¡Œå¹³ç¨³ï¼Œæ— æ˜¾è‘—å¼‚å¸¸ç‚¹â€)*

**3. æ€»ç»“ä¸å»ºè®®**
[ä¸€å¥è¯æ€»ç»“åŒºé—´è¡¨ç°åŠåç»­å…³æ³¨ç‚¹]ã€‚
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"è¯·æ ¹æ®ä»¥ä¸‹æ•°æ®ç”Ÿæˆåˆ†ææŠ¥å‘Šï¼š\n\n{data_str}"}
    ]

    payload = {
        "model": "deepseek-reasoner",
        "messages": messages,
        "stream": False
    }

    try:
        print("ğŸ¤” DeepSeek Reasoner is thinking...", end="", flush=True)
        start_time = time.time()
        resp = requests.post(f"{base_url}/chat/completions", json=payload, headers=headers)
        end_time = time.time()
        elapsed_sec = end_time - start_time
        print(f" Done. ({elapsed_sec:.2f}s)")
        
        if resp.status_code != 200:
            return f"Error from API: {resp.text}", {}
            
        data = resp.json()
        metrics = {
            "elapsed_sec": elapsed_sec,
            "usage": data.get("usage", {})
        }
        
        if "choices" in data and data["choices"]:
            choice = data["choices"][0]
            # å¯ä»¥é€‰æ‹©æ€§åœ°æ‰“å° reasoning_content
            # reasoning = choice["message"].get("reasoning_content", "")
            content = choice["message"].get("content", "")
            return content, metrics
        return "Error: No content in response.", metrics
    except Exception as e:
        return f"Error calling API: {str(e)}", {}

def analyze_point(target_date_str: str) -> Dict[str, Any]:
    """
    Run analysis for a single point in time using Execution Graph.
    """
    today = pd.Timestamp.now().normalize()
    
    if target_date_str == "yesterday":
        target_date = today - pd.Timedelta(days=1)
        date_range = "yesterday"
        
        hist_s = target_date - pd.Timedelta(days=30)
        hist_e = target_date - pd.Timedelta(days=1)
        history_range_str = f"{hist_s.strftime('%Y-%m-%d')}/{hist_e.strftime('%Y-%m-%d')}"
        
    else:
        target_date = pd.to_datetime(target_date_str, errors="raise").normalize()
        date_range = target_date.strftime("%Y-%m-%d")
        
        hist_s = target_date - pd.Timedelta(days=30)
        hist_e = target_date - pd.Timedelta(days=1)
        history_range_str = f"{hist_s.strftime('%Y-%m-%d')}/{hist_e.strftime('%Y-%m-%d')}"

    print(f"\nğŸ” Analyzing Date: {date_range} (History Baseline: {history_range_str})")

    app = build_execution_graph()

    # å®šä¹‰ä¸ yesterday_lock.py ç›¸åŒçš„ DSL åºåˆ—
    dsl_sequence = [
        {
            "id": "baseline_query",
            "tool": "query",
            "parameters": {"metric": "sales", "date_range": date_range},
        },
        {
            "id": "short_term_trend",
            "tool": "trend",
            "parameters": {
                "metric": "sales",
                "time_grain": "day",
                "compare_type": "mom",
                "date_range": date_range,
            },
        },
        {
            "id": "cycle_comparison",
            "tool": "trend",
            "parameters": {
                "metric": "sales",
                "time_grain": "day",
                "compare_type": "wow",
                "date_range": date_range,
            },
        },
        {
            "id": "anomaly_check",
            "tool": "trend",
            "parameters": {
                "metric": "sales",
                "time_grain": "day",
                "compare_type": "vs_avg",
                "date_range": history_range_str, 
            },
        },
        {
            "id": "structural_rollup",
            "tool": "rollup",
            "parameters": {
                "metric": "sales",
                "dimension": "series_group",
                "date_range": date_range,
            },
        },
        {
            "id": "pareto_scan",
            "tool": "pareto",
            "parameters": {
                "metric": "sales",
                "dimension": "series_group",
                "date_range": date_range,
            },
        },
        {
            "id": "distribution_analysis",
            "tool": "histogram",
            "parameters": {
                "metric": "datediff('day',first_assign_time,lock_time)",
                "date_range": date_range,
                "compare_date_range": history_range_str,
                "bins": 30,
            },
        }
    ]

    initial_state = {
        "dsl_sequence": dsl_sequence,
        "current_step": 0,
        "results": {},
        "signals": [],
    }

    # æ‰§è¡Œ Graph
    final_state = app.invoke(initial_state)
    
    # è®¡ç®—é£é™©ç­‰çº§
    risk_assessment = calculate_risk(final_state["signals"])

    # å‡†å¤‡ä¸Šä¸‹æ–‡æ•°æ®ä¾› Reasoner ä½¿ç”¨
    context_data = {
        "date": date_range,
        "results": final_state["results"],
        "signals": final_state["signals"],
        "risk_assessment": risk_assessment
    }
    
    return context_data

def print_metrics(metrics: Dict[str, Any]):
    if not metrics:
        return
        
    usage = metrics.get("usage", {})
    elapsed = metrics.get("elapsed_sec", 0)
    
    print("\n" + "-"*30)
    print("â±ï¸  æ€§èƒ½ç»Ÿè®¡ (Performance Metrics)")
    print("-"*30)
    print(f"â³ è¿è¡Œè€—æ—¶: {elapsed:.2f} ç§’")
    print(f"ğŸ« Token å¼€é”€:")
    print(f"   - Input Tokens: {usage.get('prompt_tokens', 0)}")
    print(f"   - Output Tokens: {usage.get('completion_tokens', 0)}")
    print(f"   - Total Tokens: {usage.get('total_tokens', 0)}")
    print("-"*30 + "\n")

def analyze_range(start_date: str, end_date: str):
    print(f"ğŸš€ Starting Reasoner Trajectory Analysis: {start_date} to {end_date}")
    
    s = pd.to_datetime(start_date)
    e = pd.to_datetime(end_date)
    dates = pd.date_range(start=s, end=e, freq='D')
    
    daily_summaries = []
    
    for d in dates:
        d_str = d.strftime("%Y-%m-%d")
        context_data = analyze_point(d_str)
        
        # ä»…æ”¶é›†æ¯æ—¥æ ¸å¿ƒæ•°æ®ï¼Œä¸ç”Ÿæˆæ¯æ—¥ç®€æŠ¥
        print(f"Processing {d_str}...", flush=True)
        
        # æå–å…³é”®æŒ‡æ ‡ä¾›åŒºé—´åˆ†æä½¿ç”¨
        baseline = context_data.get("results", {}).get("baseline_query", {})
        daily_summaries.append({
            "date": d_str,
            "core_metric": baseline,
            "signals": context_data.get("signals", [])
        })

    # æœ€åç”ŸæˆåŒºé—´æ±‡æ€»
    print("\nğŸ“š Generating Range Summary...")
    range_report, metrics = call_deepseek_reasoner({"range_data": daily_summaries}, prompt_type="range")
    print("\n" + "="*50)
    print(f"ğŸ“… åŒºé—´è½¨è¿¹æ·±åº¦ç»¼è¿° ({start_date} ~ {end_date})")
    print("="*50)
    print(range_report)
    print_metrics(metrics)

def main() -> None:
    args = _parse_args()
    
    if args.start and args.end:
        analyze_range(args.start, args.end)
    elif args.date:
        context_data = analyze_point(args.date)
        print(f"\nğŸ“ Generating Report for {args.date}...")
        report, metrics = call_deepseek_reasoner(context_data, prompt_type="daily")
        print("\n" + "="*50)
        print(f"ğŸ“Š DeepSeek Reasoner Analysis Report ({args.date})")
        print("="*50)
        print(report)
        print_metrics(metrics)
    else:
        print("Error: Please provide --date or --start and --end")

if __name__ == "__main__":
    main()
