"""
æ­¤è„šæœ¬ç”¨äºæ¼”ç¤º/æµ‹è¯•ç›®çš„ã€‚
å®ƒè·³è¿‡äº† PlanningAgent çš„è‡ªç„¶è¯­è¨€ç†è§£é˜¶æ®µï¼Œç›´æ¥æ‰§è¡Œä¸€å¥—é¢„å®šä¹‰å¥½çš„ DSL åºåˆ—ã€‚
åœºæ™¯ï¼šåˆ†æâ€œæ˜¨æ—¥è½¬åŒ–ç‡å¦‚ä½•â€æˆ–â€œä¸€æ®µæ—¶é—´å†…çš„è½¬åŒ–ç‡è½¨è¿¹â€ã€‚
ç”¨é€”ï¼šè°ƒè¯• Execution Graph æˆ– æ¼”ç¤ºæ ‡å‡†åˆ†ææµç¨‹ã€‚
"""

import os
import sys
import argparse
from typing import Dict, Any, List

import numpy as np
import pandas as pd

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from agents.execution_graph import build_execution_graph
from runtime.context import DataManager


def _safe_rate(n: float, d: float) -> float:
    return float(n / d) if d and d > 0 else 0.0


def _percent_rank(values: np.ndarray, x: float) -> float:
    n = int(values.size)
    if n == 0:
        return 0.0
    return float((values <= x).mean())


def _level_from_percentile(p: float) -> str:
    if p <= 1 / 3:
        return "ä½"
    if p <= 2 / 3:
        return "ä¸­"
    return "é«˜"


def _risk_level_from_flags(
    flag: str,
) -> str:
    if flag == "ç»“æ„æ€§å¼‚å¸¸":
        return "é«˜"
    if flag in {"é«˜æ³¢åŠ¨å¼‚å¸¸", "è¶‹åŠ¿æ€§åç¦»"}:
        return "ä¸­"
    return "ä½"


def compute_volume_stats(
    dm: DataManager,
    col: str,
    target_date: pd.Timestamp,
    history_start: pd.Timestamp,
    history_end: pd.Timestamp,
) -> dict:
    df = dm.get_assign_data()
    if df.empty or "assign_date" not in df.columns:
        return {
            "value": 0.0,
            "percentile": 0.0,
            "position": "ä½",
            "n_days": 0,
            "below_hist_min": False,
            "above_hist_max": False,
        }

    df = df.copy()
    df["assign_date"] = pd.to_datetime(df["assign_date"], errors="coerce")
    df = df[df["assign_date"].notna()]

    # Target value
    d_target = df[df["assign_date"].dt.normalize() == target_date.normalize()]
    value = float(d_target[col].sum()) if col in d_target.columns else 0.0

    # History values
    df_hist = df[(df["assign_date"] >= history_start) & (df["assign_date"] <= history_end)]
    if df_hist.empty or col not in df_hist.columns:
        hist_values = np.array([], dtype=float)
    else:
        daily = df_hist.groupby(df_hist["assign_date"].dt.normalize())[col].sum()
        hist_values = daily.fillna(0.0).astype(float).to_numpy()

    percentile = _percent_rank(hist_values, value)
    n_days = int(hist_values.size)
    hist_min = float(np.min(hist_values)) if n_days > 0 else 0.0
    hist_max = float(np.max(hist_values)) if n_days > 0 else 0.0
    below_hist_min = bool(n_days > 0 and value < hist_min)
    above_hist_max = bool(n_days > 0 and value > hist_max)
    position = _level_from_percentile(percentile)

    return {
        "value": value,
        "percentile": percentile,
        "n_days": n_days,
        "below_hist_min": below_hist_min,
        "above_hist_max": above_hist_max,
        "position": position,
    }


def compute_rate_stats(
    dm: DataManager,
    numerator_col: str,
    denominator_col: str,
    target_date: pd.Timestamp,
    history_start: pd.Timestamp,
    history_end: pd.Timestamp,
    n_min: float,
    z_high: float,
    z_mid: float,
    cv_low: float,
) -> dict:
    df = dm.get_assign_data()
    if df.empty or "assign_date" not in df.columns:
        return {
            "value": 0.0,
            "leads": 0.0,
            "mean": 0.0,
            "std": 0.0,
            "z": 0.0,
            "cv": 0.0,
            "percentile": 0.0,
            "position": "ä½",
            "anomaly_detected": False,
            "flag": "æ— æ•°æ®",
            "history_window": {"start": str(history_start.date()), "end": str(history_end.date())},
        }

    df = df.copy()
    df["assign_date"] = pd.to_datetime(df["assign_date"], errors="coerce")
    df = df[df["assign_date"].notna()]

    d_target = df[df["assign_date"].dt.normalize() == target_date.normalize()]
    leads = float(d_target[denominator_col].sum()) if denominator_col in d_target.columns else 0.0
    num = float(d_target[numerator_col].sum()) if numerator_col in d_target.columns else 0.0
    value = _safe_rate(num, leads)

    df_hist = df[(df["assign_date"] >= history_start) & (df["assign_date"] <= history_end)]
    if (
        df_hist.empty
        or numerator_col not in df_hist.columns
        or denominator_col not in df_hist.columns
        or "assign_date" not in df_hist.columns
    ):
        hist_values = np.array([], dtype=float)
    else:
        daily = (
            df_hist.groupby(df_hist["assign_date"].dt.normalize())[[numerator_col, denominator_col]]
            .sum()
            .reset_index()
        )
        denom = daily[denominator_col].replace(0, np.nan)
        hist_values = (daily[numerator_col] / denom).fillna(0.0).astype(float).to_numpy()

    mean = float(np.mean(hist_values)) if hist_values.size > 0 else 0.0
    std = float(np.std(hist_values, ddof=1)) if hist_values.size > 1 else 0.0

    if std > 0:
        z = float((value - mean) / std)
    else:
        z = 0.0

    if mean != 0:
        cv = float(abs(std / mean))
    else:
        cv = float("inf") if std > 0 else 0.0

    percentile = _percent_rank(hist_values, value)
    n_days = int(hist_values.size)
    hist_min = float(np.min(hist_values)) if n_days > 0 else 0.0
    hist_max = float(np.max(hist_values)) if n_days > 0 else 0.0
    below_hist_min = bool(n_days > 0 and value < hist_min)
    above_hist_max = bool(n_days > 0 and value > hist_max)
    percentile_resolution = float(1.0 / n_days) if n_days > 0 else 0.0
    position = _level_from_percentile(percentile)

    anomaly_detected = False
    flag = "æ­£å¸¸æ³¢åŠ¨"
    if leads < n_min:
        flag = "æ ·æœ¬ä¸è¶³"
    else:
        abs_z = abs(z)
        if abs_z >= z_high and cv < cv_low:
            anomaly_detected = True
            flag = "ç»“æ„æ€§å¼‚å¸¸"
        elif abs_z >= z_high and cv >= cv_low:
            anomaly_detected = True
            flag = "é«˜æ³¢åŠ¨å¼‚å¸¸"
        elif abs_z >= z_mid:
            anomaly_detected = True
            flag = "è¶‹åŠ¿æ€§åç¦»"

    return {
        "value": value,
        "leads": leads,
        "mean": mean,
        "std": std,
        "z": z,
        "cv": cv,
        "percentile_method": "empirical_cdf",
        "percentile": percentile,
        "n_days": n_days,
        "hist_min": hist_min,
        "hist_max": hist_max,
        "below_hist_min": below_hist_min,
        "above_hist_max": above_hist_max,
        "percentile_resolution": percentile_resolution,
        "position": position,
        "anomaly_detected": anomaly_detected,
        "flag": flag,
        "thresholds": {"n_min": n_min, "z_high": z_high, "z_mid": z_mid, "cv_low": cv_low},
        "history_window": {"start": str(history_start.date()), "end": str(history_end.date())},
    }


def _format_percentile(stats: dict) -> str:
    p = float(stats.get("percentile", 0.0))
    n = int(stats.get("n_days", 0))
    below = bool(stats.get("below_hist_min", False))
    above = bool(stats.get("above_hist_max", False))
    if n <= 0:
        return "P0.0"
    if below:
        return f"P<{(1.0 / n) * 100:.1f}ï¼ˆä½äºå†å²æœ€å°å€¼ï¼‰"
    if above:
        return f"P>{(1.0 - 1.0 / n) * 100:.1f}ï¼ˆé«˜äºå†å²æœ€å¤§å€¼ï¼‰"
    return f"P{p * 100:.1f}"

def _parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser()
    p.add_argument("--date", type=str, help="Single date to analyze (YYYY-MM-DD or 'yesterday')")
    p.add_argument("--start", type=str, help="Start date for range analysis (YYYY-MM-DD)")
    p.add_argument("--end", type=str, help="End date for range analysis (YYYY-MM-DD)")
    
    p.add_argument("--history-start-days-ago", type=int, default=60)
    p.add_argument("--history-end-days-ago", type=int, default=30)
    p.add_argument("--n-min", type=float, default=50.0)
    p.add_argument("--z-threshold", type=float, default=2.0)
    p.add_argument("--z-mid", type=float, default=1.2)
    p.add_argument("--cv-threshold", type=float, default=0.4)
    
    args = p.parse_args()
    
    if not args.date and not args.start:
        args.date = "yesterday"
        
    return args


def analyze_point(target_date_str: str, args: argparse.Namespace) -> Dict[str, Any]:
    dm = DataManager()
    today = pd.Timestamp.now().normalize()
    
    if target_date_str == "yesterday":
        target_date = today - pd.Timedelta(days=1)
        date_range = "yesterday"
    else:
        target_date = pd.to_datetime(target_date_str, errors="raise").normalize()
        date_range = target_date.strftime("%Y-%m-%d")

    history_start = target_date - pd.Timedelta(days=int(args.history_start_days_ago))
    history_end = target_date - pd.Timedelta(days=int(args.history_end_days_ago))
    
    history_range_str = f"{history_start.strftime('%Y-%m-%d')}/{history_end.strftime('%Y-%m-%d')}"

    print(f"\nğŸ” Analyzing Date: {date_range} (History Baseline: {history_range_str})")

    N_min = float(args.n_min)
    z_high = float(args.z_threshold)
    z_mid = float(args.z_mid)
    cv_low = float(args.cv_threshold)

    app = build_execution_graph()
    dsl_sequence = [
        {
            "id": "assign_leads_mom",
            "tool": "trend",
            "parameters": {
                "metric": "assign_leads",
                "time_grain": "day",
                "compare_type": "mom",
                "date_range": date_range,
            },
        },
        {
            "id": "assign_rate_7d_conversion",
            "tool": "trend",
            "parameters": {
                "metric": "assign_rate_7d_lock",
                "time_grain": "day",
                "compare_type": "mom",
                "date_range": date_range,
            },
        },
        {
            "id": "assign_rate_7d_test_drive",
            "tool": "trend",
            "parameters": {
                "metric": "assign_rate_7d_test_drive",
                "time_grain": "day",
                "compare_type": "mom",
                "date_range": date_range,
            },
        },
    ]

    initial_state = {
        "dsl_sequence": dsl_sequence,
        "current_step": 0,
        "results": {},
        "signals": [],
    }

    final_state = app.invoke(initial_state)

    conversion_stats = compute_rate_stats(
        dm=dm,
        numerator_col="ä¸‹å‘çº¿ç´¢ 7 æ—¥é”å•æ•°",
        denominator_col="ä¸‹å‘çº¿ç´¢æ•°",
        target_date=target_date,
        history_start=history_start,
        history_end=history_end,
        n_min=N_min,
        z_high=z_high,
        z_mid=z_mid,
        cv_low=cv_low,
    )
    test_drive_stats = compute_rate_stats(
        dm=dm,
        numerator_col="ä¸‹å‘çº¿ç´¢ 7 æ—¥è¯•é©¾æ•°",
        denominator_col="ä¸‹å‘çº¿ç´¢æ•°",
        target_date=target_date,
        history_start=history_start,
        history_end=history_end,
        n_min=N_min,
        z_high=z_high,
        z_mid=z_mid,
        cv_low=cv_low,
    )

    leads_stats = compute_volume_stats(
        dm=dm,
        col="ä¸‹å‘çº¿ç´¢æ•°",
        target_date=target_date,
        history_start=history_start,
        history_end=history_end,
    )

    final_state["results"]["rate_stats"] = {
        "history_window_days_ago": {
            "start_days_ago": int(args.history_start_days_ago),
            "end_days_ago": int(args.history_end_days_ago),
        },
        "params": {"N_min": N_min, "z_high": z_high, "z_mid": z_mid, "cv_low": cv_low},
        "leads_stats": leads_stats,
        "7d_conversion_rate": conversion_stats,
        "7d_test_drive_rate": test_drive_stats,
    }

    final_state["signals"].append(
        {
            "type": "volume_signal",
            "metric": "assign_leads",
            "position": leads_stats["position"],
            "percentile": leads_stats["percentile"],
            "date_range": date_range,
        }
    )

    final_state["signals"].append(
        {
            "type": "anomaly_decision",
            "metric": "7d_conversion_rate",
            "flag": conversion_stats["flag"],
            "z": conversion_stats["z"],
            "cv": conversion_stats["cv"],
            "anomaly_detected": conversion_stats["anomaly_detected"],
            "date_range": date_range,
            "history_window": conversion_stats["history_window"],
            "leads": conversion_stats["leads"],
            "position": conversion_stats["position"],
            "percentile": conversion_stats["percentile"],
        }
    )
    final_state["signals"].append(
        {
            "type": "anomaly_decision",
            "metric": "7d_test_drive_rate",
            "flag": test_drive_stats["flag"],
            "z": test_drive_stats["z"],
            "cv": test_drive_stats["cv"],
            "anomaly_detected": test_drive_stats["anomaly_detected"],
            "date_range": date_range,
            "history_window": test_drive_stats["history_window"],
            "leads": test_drive_stats["leads"],
            "position": test_drive_stats["position"],
            "percentile": test_drive_stats["percentile"],
        }
    )
    
    return final_state


def generate_assessment(state: Dict[str, Any], date_str: str, verbose: bool = True):
    results = state["results"]
    rate_stats = results.get("rate_stats", {})
    conversion_stats = rate_stats.get("7d_conversion_rate", {})
    test_drive_stats = rate_stats.get("7d_test_drive_rate", {})
    leads_stats = rate_stats.get("leads_stats", {})

    overall_risk = max(
        [
            _risk_level_from_flags(
                flag=conversion_stats.get("flag", "æ— æ•°æ®"),
            ),
            _risk_level_from_flags(
                flag=test_drive_stats.get("flag", "æ— æ•°æ®"),
            ),
        ],
        key=lambda x: {"ä½": 0, "ä¸­": 1, "é«˜": 2}.get(x, 0),
    )

    icon = {"ä½": "ğŸŸ¢", "ä¸­": "ğŸŸ¡", "é«˜": "ğŸ”´"}.get(overall_risk, "â“")
    
    reasons = []
    if conversion_stats.get("anomaly_detected"):
        reasons.append(f"è½¬åŒ–ç‡å¼‚å¸¸ (7æ—¥é”å•): {conversion_stats.get('flag')} (Z={conversion_stats.get('z',0):.2f})")
    if test_drive_stats.get("anomaly_detected"):
        reasons.append(f"è¯•é©¾ç‡å¼‚å¸¸ (7æ—¥è¯•é©¾): {test_drive_stats.get('flag')} (Z={test_drive_stats.get('z',0):.2f})")
    if leads_stats.get("position") == "é«˜" and leads_stats.get("percentile", 0) > 0.9:
        reasons.append(f"çº¿ç´¢é‡æ¿€å¢ ({_format_percentile(leads_stats)})")
    elif leads_stats.get("position") == "ä½" and leads_stats.get("percentile", 0) < 0.1:
        reasons.append(f"çº¿ç´¢é‡è¿‡ä½ ({_format_percentile(leads_stats)})")

    if verbose:
        print(f"\n{icon} [{date_str}] è½¬åŒ–ç‡ç»¼åˆè¯„ä¼°ï¼šé£é™©ç­‰çº§ï¼š{overall_risk}")
        print(
            "ğŸ“ å½“å‰ä¸‹å‘çº¿ç´¢åœ¨å†å²åˆ†å¸ƒä½ç½®ï¼š"
            f"{leads_stats.get('position', 'N/A')} ({_format_percentile(leads_stats)})"
        )
        print(
            "ğŸ“ å½“å‰7æ—¥è½¬åŒ–ç‡åœ¨å†å²åˆ†å¸ƒä½ç½®ï¼š"
            f"{conversion_stats.get('position', 'N/A')} ({_format_percentile(conversion_stats)})"
        )
        print(
            "ğŸ“ å½“å‰7æ—¥è¯•é©¾ç‡åœ¨å†å²åˆ†å¸ƒä½ç½®ï¼š"
            f"{test_drive_stats.get('position', 'N/A')} ({_format_percentile(test_drive_stats)})"
        )
        
        if reasons:
            print("   é£é™©å› å­ï¼š")
            for r in reasons:
                print(f"   - {r}")

    return {
        "date": date_str,
        "risk_level": overall_risk,
        "reasons": reasons,
        "icon": icon
    }


def analyze_range(start_date: str, end_date: str, args: argparse.Namespace):
    print(f"ğŸš€ Starting Trajectory Analysis: {start_date} to {end_date}")
    
    s = pd.to_datetime(start_date)
    e = pd.to_datetime(end_date)
    
    dates = pd.date_range(start=s, end=e, freq='D')
    
    trajectory = []
    
    for d in dates:
        d_str = d.strftime("%Y-%m-%d")
        state = analyze_point(d_str, args)
        
        # Print concise result for each day
        assessment = generate_assessment(state, d_str, verbose=True)
        trajectory.append(assessment)
        
    # Summary of trajectory
    print("\n" + "="*50)
    print(f"ğŸ“… åŒºé—´è½¨è¿¹æ±‡æ€» ({start_date} ~ {end_date})")
    print("="*50)
    
    high_risk_days = [t for t in trajectory if t['risk_level'] == 'é«˜']
    med_risk_days = [t for t in trajectory if t['risk_level'] == 'ä¸­']
    
    print(f"å…±åˆ†æ {len(trajectory)} å¤©")
    print(f"ğŸ”´ é«˜é£é™©å¤©æ•°: {len(high_risk_days)}")
    print(f"ğŸŸ¡ ä¸­é£é™©å¤©æ•°: {len(med_risk_days)}")
    
    if high_risk_days:
        print("\nâš ï¸ é«˜é£é™©æ—¥æœŸè¯¦æƒ…:")
        for t in high_risk_days:
            print(f"  - {t['date']}: {', '.join(t['reasons'])}")
            
    if not high_risk_days and not med_risk_days:
        print("\nâœ… åŒºé—´å†…è¡¨ç°å¹³ç¨³ï¼Œæ— æ˜¾è‘—å¼‚å¸¸ã€‚")


def main() -> None:
    args = _parse_args()
    
    if args.start and args.end:
        analyze_range(args.start, args.end, args)
    elif args.date:
        state = analyze_point(args.date, args)
        print("\nFinal results:")
        print(state["results"])
        print("\nSignals:")
        print(state["signals"])
        generate_assessment(state, args.date, verbose=True)
    else:
        print("Error: Please provide --date or --start and --end")


if __name__ == "__main__":
    main()
