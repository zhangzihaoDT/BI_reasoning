"""
æ­¤è„šæœ¬æ˜¯ yesterday_rate.py çš„ Reasoner å¢å¼ºç‰ˆã€‚
åŠŸèƒ½ï¼š
1) ä¿ç•™åŸæœ‰ DSLï¼ˆè½¬åŒ–ç‡ç›¸å…³çš„è¶‹åŠ¿å¯¹æ¯”ï¼‰
2) åœ¨ DSL å®Œæˆåï¼Œè®¡ç®—â€œé—¨åº—çº¿ç´¢å æ¯”â€å’Œâ€œé—¨åº—å½“æ—¥é”å•ç‡â€çš„å†å²å¯¹æ¯”ä¸æ¡ä»¶å¯¹æ¯”
3) è‹¥åˆ¤å®šä¸ºé«˜é£é™©ï¼Œè°ƒåº¦å·¥å…·ç®±ï¼ˆrollup/trendï¼‰åšç»“æ„å½’å› 
4) ä½¿ç”¨ DeepSeek Reasoner è¾“å‡ºæç®€é«˜å¯†åº¦ç®€æŠ¥
"""
import os
import sys
import argparse
import json
import time
from typing import Dict, Any, List, Tuple

import numpy as np
import pandas as pd

# Add project root to sys.path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from agents.execution_graph import build_execution_graph
from runtime.context import DataManager


def _safe_rate(n: float, d: float) -> float:
    return float(n / d) if d and d > 0 else 0.0


def _parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser()
    p.add_argument("--date", type=str, help="Single date to analyze (YYYY-MM-DD or 'yesterday')")
    p.add_argument("--start", type=str, help="Start date for range analysis (YYYY-MM-DD)")
    p.add_argument("--end", type=str, help="End date for range analysis (YYYY-MM-DD)")
    p.add_argument("--history-start-days-ago", type=int, default=60)
    p.add_argument("--history-end-days-ago", type=int, default=30)
    p.add_argument("--z-threshold", type=float, default=2.0)
    p.add_argument("--z-mid", type=float, default=1.2)
    p.add_argument("--share-window", type=float, default=0.05, help="æ¡ä»¶å¯¹æ¯”æ—¶é—¨åº—çº¿ç´¢å æ¯”çš„å®¹å¿çª—å£")
    args = p.parse_args()
    if not args.date and not args.start:
        args.date = "yesterday"
    return args


def _compute_today_and_history(dm: DataManager, target_date: pd.Timestamp, h_start: pd.Timestamp, h_end: pd.Timestamp) -> Dict[str, Any]:
    df = dm.get_assign_data().copy()
    if df.empty or "assign_date" not in df.columns:
        return {
            "today": {"leads": 0.0, "store_leads": 0.0, "store_lock_same_day": 0.0},
            "history": pd.DataFrame(columns=["assign_date", "leads", "store_leads", "store_lock_same_day"]),
        }
    df["assign_date"] = pd.to_datetime(df["assign_date"], errors="coerce")
    df = df[df["assign_date"].notna()]
    cols = ["ä¸‹å‘çº¿ç´¢æ•°", "ä¸‹å‘çº¿ç´¢æ•° (é—¨åº—)", "ä¸‹å‘çº¿ç´¢å½“æ—¥é”å•æ•° (é—¨åº—)"]
    for c in cols:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0.0)
        else:
            df[c] = 0.0
    d_target = df[df["assign_date"].dt.normalize() == target_date.normalize()]
    today = {
        "leads": float(d_target["ä¸‹å‘çº¿ç´¢æ•°"].sum()),
        "store_leads": float(d_target["ä¸‹å‘çº¿ç´¢æ•° (é—¨åº—)"].sum()),
        "store_lock_same_day": float(d_target["ä¸‹å‘çº¿ç´¢å½“æ—¥é”å•æ•° (é—¨åº—)"].sum()),
    }
    df_hist = df[(df["assign_date"] >= h_start) & (df["assign_date"] <= h_end)]
    if df_hist.empty:
        hist_df = pd.DataFrame(columns=["assign_date", "leads", "store_leads", "store_lock_same_day"])
    else:
        daily = df_hist.groupby(df_hist["assign_date"].dt.normalize())[
            ["ä¸‹å‘çº¿ç´¢æ•°", "ä¸‹å‘çº¿ç´¢æ•° (é—¨åº—)", "ä¸‹å‘çº¿ç´¢å½“æ—¥é”å•æ•° (é—¨åº—)"]
        ].sum().reset_index()
        daily.rename(columns={
            "assign_date": "assign_date",
            "ä¸‹å‘çº¿ç´¢æ•°": "leads",
            "ä¸‹å‘çº¿ç´¢æ•° (é—¨åº—)": "store_leads",
            "ä¸‹å‘çº¿ç´¢å½“æ—¥é”å•æ•° (é—¨åº—)": "store_lock_same_day",
        }, inplace=True)
        hist_df = daily
    return {"today": today, "history": hist_df}


def _percent_rank(values: np.ndarray, x: float) -> float:
    n = int(values.size)
    if n == 0:
        return 0.0
    return float((values <= x).mean())


def assess_structure_risk(stats: Dict[str, Any], z_high: float, z_mid: float) -> Dict[str, Any]:
    today = stats["today"]
    hist_df = stats["history"]
    today_share = _safe_rate(today["store_leads"], today["leads"])
    today_store_rate = _safe_rate(today["store_lock_same_day"], today["store_leads"])
    share_hist = hist_df.copy()
    share_hist["store_share"] = share_hist.apply(lambda r: _safe_rate(r["store_leads"], r["leads"]), axis=1)
    share_hist["store_rate"] = share_hist.apply(lambda r: _safe_rate(r["store_lock_same_day"], r["store_leads"]), axis=1)
    share_values = share_hist["store_share"].to_numpy(dtype=float)
    rate_values = share_hist["store_rate"].to_numpy(dtype=float)
    share_mean = float(np.mean(share_values)) if share_values.size > 0 else 0.0
    share_std = float(np.std(share_values, ddof=1)) if share_values.size > 1 else 0.0
    rate_mean = float(np.mean(rate_values)) if rate_values.size > 0 else 0.0
    rate_std = float(np.std(rate_values, ddof=1)) if rate_values.size > 1 else 0.0
    share_z = float((today_share - share_mean) / share_std) if share_std > 0 else 0.0
    rate_z = float((today_store_rate - rate_mean) / rate_std) if rate_std > 0 else 0.0
    risk_level = "ä½"
    flag = "æ­£å¸¸ç»“æ„"
    if abs(share_z) >= z_high or abs(rate_z) >= z_high:
        risk_level = "é«˜"
        flag = "ç»“æ„æ€§å¼‚å¸¸"
    elif abs(share_z) >= z_mid or abs(rate_z) >= z_mid:
        risk_level = "ä¸­"
        flag = "è¶‹åŠ¿æ€§åç¦»"
    return {
        "store_share": today_share,
        "store_rate": today_store_rate,
        "share_mean": share_mean,
        "share_std": share_std,
        "rate_mean": rate_mean,
        "rate_std": rate_std,
        "share_z": share_z,
        "rate_z": rate_z,
        "risk_level": risk_level,
        "flag": flag,
    }


def conditional_rate_assessment(stats: Dict[str, Any], window: float) -> Dict[str, Any]:
    today = stats["today"]
    hist_df = stats["history"]
    today_share = _safe_rate(today["store_leads"], today["leads"])
    hist_df = hist_df.copy()
    hist_df["store_share"] = hist_df.apply(lambda r: _safe_rate(r["store_leads"], r["leads"]), axis=1)
    hist_df["store_rate"] = hist_df.apply(lambda r: _safe_rate(r["store_lock_same_day"], r["store_leads"]), axis=1)
    lower = max(0.0, today_share - window)
    upper = min(1.0, today_share + window)
    cond = hist_df[(hist_df["store_share"] >= lower) & (hist_df["store_share"] <= upper)]
    cond_values = cond["store_rate"].to_numpy(dtype=float)
    cond_mean = float(np.mean(cond_values)) if cond_values.size > 0 else 0.0
    cond_std = float(np.std(cond_values, ddof=1)) if cond_values.size > 1 else 0.0
    today_store_rate = _safe_rate(today["store_lock_same_day"], today["store_leads"])
    cond_z = float((today_store_rate - cond_mean) / cond_std) if cond_std > 0 else 0.0
    return {
        "window": window,
        "share_lower": lower,
        "share_upper": upper,
        "conditional_mean": cond_mean,
        "conditional_std": cond_std,
        "today_store_rate": today_store_rate,
        "conditional_z": cond_z,
        "n_days": int(cond_values.size),
    }


def _build_dsl(date_range: str) -> List[Dict[str, Any]]:
    return [
        {
            "id": "assign_leads_mom",
            "tool": "trend",
            "parameters": {"metric": "assign_leads", "time_grain": "day", "compare_type": "mom", "date_range": date_range},
        },
        {
            "id": "assign_rate_7d_lock",
            "tool": "trend",
            "parameters": {"metric": "assign_rate_7d_lock", "time_grain": "day", "compare_type": "mom", "date_range": date_range},
        },
        {
            "id": "assign_rate_7d_test_drive",
            "tool": "trend",
            "parameters": {"metric": "assign_rate_7d_test_drive", "time_grain": "day", "compare_type": "mom", "date_range": date_range},
        },
    ]


def _toolbox_for_high_risk(date_range: str, compare_date_range: str = None) -> List[Dict[str, Any]]:
    tasks = [
        # 1. ç»“æ„åˆ†å¸ƒ (ä»…ä¿ç•™è½¦å‹ Seriesï¼Œå› å…¶å¯¹äº§å“ç­–ç•¥å½±å“æœ€å¤§)
        {
            "id": "sales_dist_by_series",
            "tool": "distribution",
            "parameters": {
                "metric": "sales", 
                "dimension": "series_group", 
                "date_range": date_range,
                "compare_date_range": compare_date_range
            },
        },
        # 2. é”€é‡è¶‹åŠ¿ (ä¿ç•™ 30 å¤©è¶‹åŠ¿ä»¥è¯†åˆ«å½¢æ€)
        {
            "id": "sales_trend_30d",
            "tool": "trend",
            "parameters": {"metric": "sales", "date_range": "last_30_days", "time_grain": "day"},
        },
        # 3. æ ¸å¿ƒè½¬åŒ–ç‡åˆ†å¸ƒå®šä½ (è¿‘ 365 å¤©åˆ†å¸ƒï¼Œå®šä½å½“å‰æ°´ä½)
        {
            "id": "rate_dist_30d",
            "tool": "distribution",
            "parameters": {"metric": "assign_store_structure", "date_range": "yesterday", "compare_date_range": "last_365_days", "bins": 20, "return_buckets": False},
        },
        {
            "id": "rate_dist_store_share_30d",
            "tool": "distribution",
            "parameters": {"metric": "assign_store_leads_ratio", "date_range": "yesterday", "compare_date_range": "last_365_days", "bins": 20, "return_buckets": False},
        },
        {
            "id": "rate_dist_7d_lock_30d",
            "tool": "distribution",
            "parameters": {"metric": "assign_rate_7d_lock", "date_range": "yesterday", "compare_date_range": "last_365_days", "bins": 20, "return_buckets": False},
        },
        {
            "id": "rate_dist_7d_drive_30d",
            "tool": "distribution",
            "parameters": {"metric": "assign_rate_7d_test_drive", "date_range": "yesterday", "compare_date_range": "last_365_days", "bins": 20, "return_buckets": False},
        },
        # 4. é—¨åº—çº¿ç´¢æ•°ç¯æ¯” (ç”¨äºå½’å› æ€»çº¿ç´¢å˜åŒ–)
        {
            "id": "assign_trend_store_leads",
            "tool": "trend",
            "parameters": {"metric": "assign_store_leads", "time_grain": "day", "compare_type": "mom", "date_range": date_range},
        },
    ]
    return tasks


def _get_wow_tasks(date_range: str) -> List[Dict[str, Any]]:
    return [
        # 5. é—¨åº—çº¿ç´¢æ•°åŒæ¯” (å‘¨åŒæ¯”)
        {
            "id": "assign_trend_store_leads_wow",
            "tool": "trend",
            "parameters": {"metric": "assign_store_leads", "time_grain": "day", "compare_type": "wow", "date_range": date_range},
        },
        # 6. æ€»çº¿ç´¢æ•°åŒæ¯” (å‘¨åŒæ¯”)
        {
            "id": "assign_trend_leads_wow",
            "tool": "trend",
            "parameters": {"metric": "assign_leads", "time_grain": "day", "compare_type": "wow", "date_range": date_range},
        },
    ]


def _call_deepseek_reasoner(payload: Dict[str, Any]) -> Tuple[str, Dict[str, Any]]:
    api_key = None
    env_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), ".env")
    if os.path.exists(env_path):
        with open(env_path, "r", encoding="utf-8") as f:
            for line in f:
                if line.strip() and not line.startswith("#"):
                    if "deepseek=" in line or "deepseek =" in line:
                        api_key = line.split("=", 1)[1].strip()
                        break
    if not api_key:
        return "âš ï¸ Error: DeepSeek API Key not found in .env", {}
    base_url = "https://api.deepseek.com"
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    system_prompt = (
        "ä½ æ˜¯ä¸€ä½é«˜å¯†åº¦ä¸šåŠ¡è¯Šæ–­ä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„ JSON æ•°æ®å­—å…¸ï¼ˆåŒ…å« keys: 'core', 'sales_orders', 'leads_trend', 'rate_trend', 'signals'ï¼‰ï¼Œè¾“å‡ºä¸€ä»½**æç®€ã€å»å™ªã€é«˜å¯†åº¦**çš„è¯Šæ–­æŠ¥å‘Šã€‚\n"
        "æ•°æ®æºæ˜ å°„è¯´æ˜ï¼š\n"
        "- **core**: åŒ…å«æ ¸å¿ƒè½¬åŒ–ç‡æŒ‡æ ‡ (assign_store_structure, å³**é—¨åº—çº¿ç´¢å½“æ—¥é”å•ç‡**) åŠæ¸ é“ç»“æ„æŒ‡æ ‡ (assign_store_leads_ratio, å³**é—¨åº—çº¿ç´¢å æ¯”**) çš„å†å²ç»Ÿè®¡ (Z-score)ã€‚\n"
        "- **sales_orders**: \n"
        "    - **structure**: åŒ…å«ç»“æ„åˆ†å¸ƒ (series) åŠ SAD å¼‚åŠ¨è¯„åˆ†ã€‚\n"
        "    - **trend**: åŒ…å«é”€é‡è¶‹åŠ¿ (day_30) åŠç”Ÿå‘½å‘¨æœŸä¿¡å· (lifecycle)ã€‚**æ³¨æ„ï¼šé”€é‡æ—¥ç¯æ¯”å˜åŒ–å¿…é¡»ä½¿ç”¨ `yesterday_change` å­—æ®µä¸­çš„æ•°æ®ï¼Œä¸¥ç¦è‡ªè¡Œæ ¹æ®ä¸å®Œæ•´çš„ `series` åˆ—è¡¨æœ«ç«¯è®¡ç®—ï¼Œé˜²æ­¢å› æ•°æ®æˆªæ–­å¯¼è‡´è¯¯åˆ¤ã€‚**\n"
        "- **leads_trend**: åŒ…å«çº¿ç´¢é‡è¶‹åŠ¿ã€‚\n"
        "    - **total_leads**: æ€»çº¿ç´¢æ•° (assign_leads) çš„ç¯æ¯”å˜åŒ– (MoM/DoD)ã€‚\n"
        "    - **store_leads**: é—¨åº—çº¿ç´¢æ•° (assign_store_leads) çš„ç¯æ¯”å˜åŒ–ã€‚\n"
        "    - **leads_wow**: æ€»çº¿ç´¢æ•° (assign_leads) çš„å‘¨åŒæ¯”å˜åŒ– (WoW)ã€‚\n"
        "    - **store_leads_wow**: é—¨åº—çº¿ç´¢æ•° (assign_store_leads) çš„å‘¨åŒæ¯”å˜åŒ– (WoW)ã€‚\n"
        "- **rate_trend**: åŒ…å« 3 ç»„è½¬åŒ–ç‡åŠ 1 ç»„ç»“æ„å æ¯”åœ¨è¿‘ 365 å¤©å†å²åˆ†å¸ƒä¸­çš„å®šä½ï¼ˆDistribution Checkï¼‰ï¼š\n"
        "    - **30d**: é—¨åº—çº¿ç´¢å½“æ—¥é”å•ç‡ (assign_store_structure)\n"
        "    - **store_share_30d**: é—¨åº—çº¿ç´¢å æ¯” (assign_store_leads_ratio)\n"
        "    - **7d_lock_30d**: 7æ—¥é”å•ç‡ (assign_rate_7d_lock)\n"
        "    - **7d_drive_30d**: 7æ—¥è¯•é©¾ç‡ (assign_rate_7d_test_drive)\n"
        "- **signals**: åŒ…å«ç³»ç»Ÿè‡ªåŠ¨è¯†åˆ«çš„å¼‚å¸¸ä¿¡å·ã€‚\n\n"
        "ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ ¼å¼å’ŒåŸåˆ™ï¼š\n"
        "1. **æ ¼å¼æ¨¡æ¿**ï¼š\n"
        "## ğŸŸ¢/ğŸ”´ è¯Šæ–­ç»“è®ºï¼šé£é™© [Low/High]\n"
        "**æ ¸å¿ƒæ•°æ®**ï¼š[åŸºäº core æ•°æ®ï¼ŒæŒ‡å‡ºè½¬åŒ–ç‡ç»å¯¹å€¼åŠåç¦»åº¦ï¼Œå¦‚â€œé—¨åº—çº¿ç´¢å½“æ—¥é”å•ç‡0.75% (Z-score -2.44)â€ï¼›è‹¥é—¨åº—çº¿ç´¢å æ¯”æœ‰æ˜¾è‘—åç¦»ä¹Ÿéœ€æŒ‡å‡ºï¼Œå¦‚â€œé—¨åº—çº¿ç´¢å æ¯”æ¿€å¢(Z=3.1)â€]ã€‚\n"
        "**é£é™©åˆ¤å®š**ï¼š[ä¸€å¥è¯å®šæ€§ï¼Œå¦‚â€œé—¨åº—çº¿ç´¢å½“æ—¥é”å•ç‡æ˜¾è‘—ä½äºå†å²å‡å€¼ï¼Œæ„æˆç»“æ„æ€§å¼‚å¸¸â€]ã€‚\n\n"
        "## ğŸ” é€é¡¹æ’æŸ¥ (Checklist)\n"
        "è¯·æŒ‰ä»¥ä¸‹é¡ºåºé€é¡¹æ£€æŸ¥ï¼Œ**ä»…å±•ç¤ºæœ‰é—®é¢˜ï¼ˆHigh Riskï¼‰çš„é¡¹**ï¼Œè‹¥æŸé¡¹æ­£å¸¸ï¼ˆå¦‚æ³¢åŠ¨åœ¨åˆç†èŒƒå›´å†…ï¼‰åˆ™**ç›´æ¥çœç•¥**ï¼Œä¿æŒæŠ¥å‘Šæç®€ã€‚\n"
        "**1. ç»“æ„åç§» (Structure Check)**ï¼š[æ£€æŸ¥ sales_orders.structureã€‚è‹¥ SAD > 0.1ï¼ŒæŒ‡å‡ºå…·ä½“çš„åç§»å› å­ã€‚ä¾‹ï¼šâ€œè½¦å‹ç»“æ„åç§»(SAD=0.34)ï¼Œä¸»å›  LS9 å æ¯”å›è½(-14pct)è¢« CM2(+13pct)æŒ¤å ã€‚â€]\n"
        "**2. è¶‹åŠ¿æ–­å±‚ (Sales Trend Check)**ï¼š[æ£€æŸ¥ sales_orders.trendã€‚è§‚å¯Ÿ 30 å¤©è¶‹åŠ¿çº¿ï¼Œè‹¥å‘ˆç°æ€¥å‰§ä¸‹è¡Œæˆ–å¤„äºä½ä½ï¼ŒæŒ‡å‡ºå…·ä½“å½¢æ€ã€‚å¼•ç”¨ç¯æ¯”è·Œå¹…æ—¶åŠ¡å¿…ä½¿ç”¨ `yesterday_change` å­—æ®µã€‚ä¾‹ï¼šâ€œLS9 é”€é‡å¤„äºä¸Šå¸‚é€€å¡åçš„ä½ä½éœ‡è¡æœŸï¼Œæ—¥ç¯æ¯”å¾®è·Œ 5%ã€‚â€]\n"
        "**3. è½¬åŒ–ç‡æ°´ä½ (Rate Position Check)**ï¼š[æ£€æŸ¥ rate_trend ä¸­çš„åˆ†å¸ƒå®šä½ (position)ã€‚è‹¥ä»»ä¸€æŒ‡æ ‡å¤„äºå†å²ä½ä½ï¼ˆå¦‚ P<10 æˆ– ä½äºå†å²å‡å€¼-1Ïƒï¼‰ï¼Œæ˜ç¡®æŒ‡å‡ºç™¾åˆ†ä½ (Percentile)ã€‚ç‰¹åˆ«æ³¨æ„é—¨åº—çº¿ç´¢å æ¯” (store_share_30d) çš„å¼‚å¸¸åˆ†å¸ƒã€‚ä¾‹ï¼šâ€œé—¨åº—çº¿ç´¢å½“æ—¥é”å•ç‡ä¸7æ—¥è¯•é©¾ç‡å‡å¤„äºå†å²æä½æ°´ä½(P<5)ï¼Œè¡¨æ˜æµé‡è´¨é‡æˆ–æ‰¿æ¥èƒ½åŠ›å‡ºç°ç³»ç»Ÿæ€§ä¸‹æ»‘ã€‚â€]\n"
        "**4. çº¿ç´¢å½’å›  (Leads Impact Check)**ï¼š[æ£€æŸ¥ leads_trendã€‚è‹¥æ€»çº¿ç´¢ (total_leads) æˆ– é—¨åº—çº¿ç´¢ (store_leads) ä»»ä¸€å‘ç”Ÿæ˜¾è‘—æ³¢åŠ¨ï¼ˆå¦‚è·Œå¹… > 10%ï¼‰ï¼Œåˆ™å¿…é¡»è¿›è¡Œå½’å› åˆ†æã€‚æ£€æŸ¥æ€»çº¿ç´¢æ³¢åŠ¨æ˜¯å¦ç”±é—¨åº—çº¿ç´¢å¯¼è‡´ï¼Œå¹¶å¯¹æ¯” WoW æ•°æ® (leads_wow, store_leads_wow) ç¡®è®¤æ˜¯å¦ä¸ºå‘¨æœŸæ€§æ³¢åŠ¨ã€‚ä¾‹ï¼šâ€œæ€»çº¿ç´¢é‡ç¯æ¯”ä¸‹è·Œ 4%ï¼Œä½†é—¨åº—çº¿ç´¢å¤§å¹…èç¼© (-26%) ä¸” WoW åŒæ­¥ä¸‹è·Œ 20%ï¼Œè¡¨æ˜éå‘¨æœŸæ€§çš„æ¸ é“å¼‚å¸¸ã€‚â€]\n\n"
        "## ğŸ’¡ å½’å› ç»¼è¿°\n"
        "[åŸºäºä¸Šè¿°æ£€å‡ºçš„å¼‚å¸¸é¡¹ï¼Œç”¨ä¸€å¥è¯é€»è¾‘é—­ç¯è§£é‡Šæ ¸å¿ƒè½¬åŒ–ç‡å¼‚å¸¸çš„åŸå› ã€‚ä¾‹ï¼šâ€œLS9 ä¸Šå¸‚é€€å¡å¯¼è‡´é«˜è½¬åŒ–å®¢ç¾¤æµå¤±ï¼Œå åŠ é•¿æœŸè½¬åŒ–ç‡ä¸‹è¡Œè¶‹åŠ¿ï¼Œå¯¼è‡´ä»Šæ—¥è½¬åŒ–ç‡å‡»ç©¿å†å²æå€¼ã€‚â€]\n\n"
        "2. **åŸåˆ™**ï¼š\n"
        "**æœ‰é—®é¢˜è¯´ï¼Œæ²¡é—®é¢˜ä¸è¯´**ï¼šä¸è¦ç½—åˆ—æ­£å¸¸æ•°æ®ï¼Œåªæš´éœ²é£é™©ã€‚\n"
        "**é‡åŒ–ä¼˜å…ˆ**ï¼šç¦æ­¢ä½¿ç”¨â€œå¤§å¹…ä¸Šå‡â€ç­‰æ¨¡ç³Šè¯ï¼Œå¿…é¡»ä½¿ç”¨â€œä½-2.44Ïƒâ€ã€â€œSAD 0.33â€ç­‰ç²¾ç¡®æ•°æ®ã€‚\n"
        "**é€»è¾‘é—­ç¯**ï¼šæœ€åçš„å½’å› ç»¼è¿°å¿…é¡»åŸºäº Checklist ä¸­å‘ç°çš„é—®é¢˜ã€‚"
    )
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": json.dumps(payload, ensure_ascii=False, indent=2, default=str)},
    ]
    req = {"model": "deepseek-reasoner", "messages": messages, "stream": False}
    try:
        print("ğŸ¤” DeepSeek Reasoner is thinking...", end="", flush=True)
        t0 = time.time()
        import requests
        resp = requests.post(f"{base_url}/chat/completions", json=req, headers=headers)
        t1 = time.time()
        print(f" Done. ({t1 - t0:.2f}s)")
        if resp.status_code != 200:
            return f"Error from API: {resp.text}", {}
        data = resp.json()
        usage = data.get("usage", {})
        content = ""
        if "choices" in data and data["choices"]:
            content = data["choices"][0]["message"].get("content", "")
        return content, {"elapsed_sec": t1 - t0, "usage": usage}
    except Exception as e:
        return f"Error calling API: {str(e)}", {}


def analyze_point(target_date_str: str, args: argparse.Namespace) -> Dict[str, Any]:
    dm = DataManager()
    today = pd.Timestamp.now().normalize()
    if target_date_str == "yesterday":
        target_date = today - pd.Timedelta(days=1)
        date_range = "yesterday"
    else:
        target_date = pd.to_datetime(target_date_str, errors="raise").normalize()
        date_range = target_date.strftime("%Y-%m-%d")
    h_start = target_date - pd.Timedelta(days=int(args.history_start_days_ago))
    h_end = target_date - pd.Timedelta(days=int(args.history_end_days_ago))
    history_range_str = f"{h_start.strftime('%Y-%m-%d')}/{h_end.strftime('%Y-%m-%d')}"
    print(f"\nğŸ” Analyzing Date: {date_range} (History Baseline: {history_range_str})")
    app = build_execution_graph()
    state = {
        "dsl_sequence": _build_dsl(date_range),
        "current_step": 0,
        "results": {},
        "signals": [],
    }
    final_state = app.invoke(state)
    stats = _compute_today_and_history(dm, target_date, h_start, h_end)
    structure_risk = assess_structure_risk(stats, z_high=float(args.z_threshold), z_mid=float(args.z_mid))
    conditional = conditional_rate_assessment(stats, window=float(args.share_window))
    lifecycle_finish_signals = []
    try:
        with open("/Users/zihao_/Documents/github/W52_reasoning/world/business_definition.json", "r", encoding="utf-8") as f:
            config = json.load(f)
        tps = config.get("time_periods", {})
        for series, period in tps.items():
            end_str = period.get("end")
            finish_str = period.get("finish")
            if not end_str or not finish_str:
                continue
            end_date = pd.to_datetime(end_str, errors="coerce")
            finish_date = pd.to_datetime(finish_str, errors="coerce")
            if pd.isna(end_date) or pd.isna(finish_date):
                continue
            if finish_date.normalize() + pd.Timedelta(days=31) <= target_date <= finish_date.normalize() + pd.Timedelta(days=60):
                days_since_finish = int((target_date - finish_date.normalize()).days)
                lifecycle_finish_signals.append({
                    "type": "lifecycle_finish_window",
                    "series": series,
                    "end": str(end_date.date()),
                    "finish": str(finish_date.date()),
                    "days_since_finish": days_since_finish,
                    "date_range": date_range,
                })
    except Exception:
        pass
    final_state["results"]["assign_structure"] = {
        "today": stats["today"],
        "history_window": {"start": str(h_start.date()), "end": str(h_end.date())},
        "structure_risk": structure_risk,
        "conditional": conditional,
    }
    final_state["signals"].append(
        {
            "type": "structure_anomaly",
            "metric": "assign_store_structure",
            "risk_level": structure_risk["risk_level"],
            "share_z": structure_risk["share_z"],
            "rate_z": structure_risk["rate_z"],
            "flag": structure_risk["flag"],
            "date_range": date_range,
        }
    )
    final_state["signals"].extend(lifecycle_finish_signals)
    if structure_risk["risk_level"] == "é«˜":
        toolbox = _toolbox_for_high_risk(date_range, history_range_str)
        print("âš™ï¸ é«˜é£é™©è§¦å‘ï¼šè°ƒåº¦å·¥å…·ç®±è¿›è¡Œæ’æŸ¥")
        state2 = {
            "dsl_sequence": toolbox,
            "current_step": 0,
            "results": {},
            "signals": [],
        }
        deep_state = app.invoke(state2)
        final_state["results"]["toolbox_analysis"] = deep_state["results"]

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘ WoW å‘¨æœŸæ€§æ’æŸ¥
        # è§¦å‘æ¡ä»¶ï¼šé—¨åº—çº¿ç´¢ç¯æ¯”å˜åŒ–å¹…åº¦ >= 10%
        store_leads_res = deep_state["results"].get("assign_trend_store_leads", {})
        change_pct = store_leads_res.get("change_pct", 0.0)
        
        if abs(change_pct) >= 0.1:
            print(f"âš ï¸ æ£€æµ‹åˆ°é—¨åº—çº¿ç´¢æ˜¾è‘—æ³¢åŠ¨ ({change_pct:.1%})ï¼Œè¿½åŠ  WoW å‘¨æœŸæ€§æ’æŸ¥...")
            wow_tasks = _get_wow_tasks(date_range)
            state3 = {
                "dsl_sequence": wow_tasks,
                "current_step": 0,
                "results": {},
                "signals": [],
            }
            wow_state = app.invoke(state3)
            # Merge results
            final_state["results"]["toolbox_analysis"].update(wow_state["results"])

    # Group results for DeepSeek
    sales_structure = {}
    sales_trend = {}
    rate_trend = {}
    leads_trend = {}
    
    # 0. Add initial DSL results to leads_trend
    if "assign_leads_mom" in final_state["results"]:
        leads_trend["total_leads"] = final_state["results"]["assign_leads_mom"]
    
    # 1. Add lifecycle check to sales_trend
    if "lifecycle_check" in final_state["results"]:
        sales_trend["lifecycle"] = final_state["results"]["lifecycle_check"]

    # 3. Split toolbox results if available
    if "toolbox_analysis" in final_state["results"]:
        for k, v in final_state["results"]["toolbox_analysis"].items():
            if k.startswith("sales_dist_"):
                clean_key = k.replace("sales_dist_by_", "")
                sales_structure[clean_key] = v
            elif k.startswith("sales_trend_"):
                clean_key = k.replace("sales_trend_", "")
                sales_trend[clean_key] = v
            elif k.startswith("rate_dist_"):
                clean_key = k.replace("rate_dist_", "")
                rate_trend[clean_key] = v
            elif k.startswith("assign_trend_"):
                clean_key = k.replace("assign_trend_", "")
                leads_trend[clean_key] = v
            else:
                pass

    payload = {
        "date": date_range,
        "core": final_state["results"].get("assign_structure", {}),
        "sales_orders": {
            "structure": sales_structure,
            "trend": sales_trend
        },
        "leads_trend": leads_trend,
        "rate_trend": rate_trend,
        "signals": final_state["signals"],
    }
    
    report, metrics = _call_deepseek_reasoner(payload)
    final_state["results"]["reasoner_report"] = report
    final_state["results"]["reasoner_metrics"] = metrics
    return final_state


def main() -> None:
    args = _parse_args()
    if args.start and args.end:
        s = pd.to_datetime(args.start)
        e = pd.to_datetime(args.end)
        dates = pd.date_range(start=s, end=e, freq="D")
        for d in dates:
            d_str = d.strftime("%Y-%m-%d")
            state = analyze_point(d_str, args)
            print("\n" + "=" * 50)
            print(f"ğŸ“Š Assign Structure Reasoner Report ({d_str})")
            print("=" * 50)
            print(state["results"].get("reasoner_report", ""))
    elif args.date:
        state = analyze_point(args.date, args)
        print("\n" + "=" * 50)
        print(f"ğŸ“Š Assign Structure Reasoner Report ({args.date})")
        print("=" * 50)
        print(state["results"].get("reasoner_report", ""))
        m = state["results"].get("reasoner_metrics", {})
        if m:
            usage = m.get("usage", {})
            print("\n------------------------------")
            print("â±ï¸  æ€§èƒ½ç»Ÿè®¡ (Performance Metrics)")
            print("------------------------------")
            print(f"â³ è¿è¡Œè€—æ—¶: {float(m.get('elapsed_sec', 0)):.2f} ç§’")
            print("ğŸ« Token å¼€é”€:")
            print(f"   - Input Tokens: {usage.get('prompt_tokens', 0)}")
            print(f"   - Output Tokens: {usage.get('completion_tokens', 0)}")
            print(f"   - Total Tokens: {usage.get('total_tokens', 0)}")
            print("------------------------------")
    else:
        print("Error: Please provide --date or --start and --end")


if __name__ == "__main__":
    main()
